{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_data(train_path, test_path):\n",
        "    train_data = pd.read_csv(train_path)\n",
        "    test_data = pd.read_csv(test_path)\n",
        "\n",
        "    #I concated them to have a uniform data set\n",
        "    all_data = pd.concat([train_data, test_data], axis=0)\n",
        "\n",
        "    #Dropped columns I dont think I need\n",
        "    all_data = all_data.drop(['id', 'Surname', 'CustomerId'], axis=1)\n",
        "\n",
        "    #variables\n",
        "    all_data = pd.get_dummies(all_data, columns=['Geography', 'Gender'], drop_first=True)\n",
        "\n",
        "    # I kinda tweaked some of the features\n",
        "    # noticed that the accuracy is rlly bad without doing additional operations\n",
        "    all_data['BalanceByEstimatedSalary'] = all_data['Balance'] / (all_data['EstimatedSalary'] + 1)\n",
        "    all_data['ProductsPerTenure'] = all_data['NumOfProducts'] / (all_data['Tenure'] + 1)\n",
        "\n",
        "    #Normalize numerical features\n",
        "    numerical_features = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary', 'BalanceByEstimatedSalary', 'ProductsPerTenure']\n",
        "    for feature in numerical_features:\n",
        "        all_data[feature] = (all_data[feature] - all_data[feature].mean()) / all_data[feature].std()\n",
        "\n",
        "    #Took them apart (test and train)\n",
        "    X = all_data[all_data['Exited'].notna()].drop('Exited', axis=1)\n",
        "    y = all_data[all_data['Exited'].notna()]['Exited']\n",
        "    X_test = all_data[all_data['Exited'].isna()].drop('Exited', axis=1)\n",
        "\n",
        "    return X, y, X_test\n",
        "\n",
        "def feature_selection(X, y, k):\n",
        "    correlations = []\n",
        "    for column in X.columns:\n",
        "        #using the correlation to select the features\n",
        "        corr = np.corrcoef(X[column], y)[0, 1]\n",
        "        correlations.append((abs(corr), column))\n",
        "\n",
        "    correlations.sort(reverse=True)\n",
        "    selected_features = [corr[1] for corr in correlations[:k]]\n",
        "\n",
        "    return X[selected_features], selected_features\n",
        "\n",
        "class KNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._predict(x) for x in X.values])\n",
        "\n",
        "    def _predict(self, x):\n",
        "        distances = self.compute_distances(x)\n",
        "        k_indices = np.argsort(distances)[:self.k]\n",
        "        k_nearest_labels = self.y_train.iloc[k_indices]\n",
        "\n",
        "        #it might return an error if I dont add the 1e-5\n",
        "        weights = 1/(distances[k_indices] + 1e-5)\n",
        "        weighted_votes = np.bincount(k_nearest_labels, weights=weights)\n",
        "        most_common = weighted_votes.argmax()\n",
        "        return most_common\n",
        "\n",
        "    def compute_distances(self, x):\n",
        "        # sticking with euclidian because i noticed mahataan makes minor difference\n",
        "        X_train_values = self.X_train.values.astype(np.float64)\n",
        "        x_values = np.array(x, dtype=np.float64)\n",
        "        differences = X_train_values - x_values\n",
        "        squared_differences = differences ** 2\n",
        "        summed_differences = np.sum(squared_differences, axis=1)\n",
        "        #additional operation to prevent error\n",
        "        return np.sqrt(summed_differences)\n",
        "\n",
        "def accuracy_score(y_true, y_pred):\n",
        "    return np.mean(y_true == y_pred)\n",
        "\n",
        "def cross_validate(X, y, knn, n_splits=5):\n",
        "    #reintialize the data\n",
        "    indices = np.arange(len(X))\n",
        "    np.random.shuffle(indices)\n",
        "    fold_size = len(X) // n_splits\n",
        "    scores = []\n",
        "\n",
        "    for i in range(n_splits):\n",
        "        #iterate through the folds\n",
        "        start = i * fold_size\n",
        "        end = start + fold_size if i < n_splits - 1 else len(X)\n",
        "        val_indices = indices[start:end]\n",
        "        train_indices = np.concatenate([indices[:start], indices[end:]])\n",
        "\n",
        "        X_train, X_val = X.iloc[train_indices], X.iloc[val_indices]\n",
        "        y_train, y_val = y.iloc[train_indices], y.iloc[val_indices]\n",
        "\n",
        "        knn.fit(X_train, y_train)\n",
        "        y_pred = knn.predict(X_val)\n",
        "\n",
        "        accuracy = accuracy_score(y_val, y_pred)\n",
        "        scores.append(accuracy)\n",
        "    # return the result\n",
        "    return np.mean(scores)\n",
        "\n",
        "def select_best_model(X, y, k_neighbors, n_features):\n",
        "    best_score = 0\n",
        "    best_k = 0\n",
        "    best_n_features = 0\n",
        "\n",
        "    for k in k_neighbors:\n",
        "        for n in n_features:\n",
        "            X_selected, _ = feature_selection(X, y, n)\n",
        "            knn = KNN(k=k)\n",
        "            score = cross_validate(X_selected, y, knn)\n",
        "            print(f\"k={k}, n_features={n}, Mean Accuracy={score}\")\n",
        "\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_k = k\n",
        "                best_n_features = n\n",
        "    # by the time we reach here, we know what is our best mode;\n",
        "    return best_k, best_n_features, best_score\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Data loading\n",
        "    X, y, X_test = preprocess_data('train.csv', 'test.csv')\n",
        "\n",
        "    # picked a few that are representative, ran a lot more test than these\n",
        "    k_values = [3, 7, 9, 11]\n",
        "    n_features = [5,  7, 9]\n",
        "    best_k, best_n_features, best_score = select_best_model(X, y, k_values, n_features)\n",
        "    print(f\"Best k: {best_k}\")\n",
        "    print(f\"Best number of features: {best_n_features}\")\n",
        "    print(f\"Best Accuracy score: {best_score}\")\n",
        "\n",
        "    # Train the final model on training dataset\n",
        "    X_selected, selected_features = feature_selection(X, y, best_n_features)\n",
        "    final_knn = KNN(k=best_k)\n",
        "    final_knn.fit(X_selected, y)\n",
        "    # prediction\n",
        "    X_test_selected = X_test[selected_features]\n",
        "    test_predictions = final_knn.predict(X_test_selected)\n",
        "\n",
        "    #saving to csv\n",
        "    test_data = pd.read_csv('test.csv')\n",
        "    pd.DataFrame({'id': test_data['id'], 'Exited': test_predictions}).to_csv('submissions.csv', index=False)\n",
        "\n",
        "    print(\"Predictions saved to 'submissions.csv'\")\n",
        "    print(\"Selected features:\", selected_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbQ4BnZfd7Dn",
        "outputId": "3f4f9866-6c7f-4b1d-8451-14fe1ea4e705"
      },
      "execution_count": 25,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "k=3, n_features=5, Mean Accuracy=0.8726666666666667\n",
            "k=3, n_features=7, Mean Accuracy=0.8688666666666667\n",
            "k=3, n_features=9, Mean Accuracy=0.8689333333333333\n",
            "k=7, n_features=5, Mean Accuracy=0.8794666666666666\n",
            "k=7, n_features=7, Mean Accuracy=0.8788666666666666\n",
            "k=7, n_features=9, Mean Accuracy=0.8851333333333334\n",
            "k=9, n_features=5, Mean Accuracy=0.881\n",
            "k=9, n_features=7, Mean Accuracy=0.8792666666666665\n",
            "k=9, n_features=9, Mean Accuracy=0.8854666666666666\n",
            "k=11, n_features=5, Mean Accuracy=0.8832666666666666\n",
            "k=11, n_features=7, Mean Accuracy=0.8833333333333334\n",
            "k=11, n_features=9, Mean Accuracy=0.8888\n",
            "Best k: 11\n",
            "Best number of features: 9\n",
            "Best Accuracy score: 0.8888\n",
            "Predictions saved to 'submissions.csv'\n",
            "Selected features: ['Age', 'NumOfProducts', 'Geography_Germany', 'IsActiveMember', 'Gender_Male', 'Balance', 'ProductsPerTenure', 'Geography_Spain', 'CreditScore']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cs506",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}